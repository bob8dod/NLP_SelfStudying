{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Subword Tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "O5gG4Cm2FXWF"
      ],
      "authorship_tag": "ABX9TyMDYIJ5cK1AavSZ4R8i+gcx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bob8dod/NLP_SelfStudying/blob/main/Subword%20Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCzuytFRWxd5"
      },
      "source": [
        "**서브워드 토크나이저(Subword Tokenizer)**\n",
        "- 단어 토큰화에서 더 나아가 단어를 서브워드 단위까지 나누는 서브워드 토큰화\n",
        "- OOV나 희귀 단어, 신조어와 같은 문제를 완화시킬 수 있음\n",
        "- 하나의 단어를 여러 서브워드로 분리해서 단어를 인코딩 및 임베딩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5gG4Cm2FXWF"
      },
      "source": [
        "# 센텐스피스(SentencePiece)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOy7fJjtFbjC"
      },
      "source": [
        "BPE 알고리즘과 Unigram Language Model Tokenizer를 구현한 센텐스피스  \n",
        "사전 토큰화 작업없이 단어 분리 토큰화를 수행하므로 언어에 종속되지 않음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndcH4DITEnhJ",
        "outputId": "6d56e946-28d9-46ad-b630-119dc1d65e17"
      },
      "source": [
        "! pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 7.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEOztcBiGEtJ"
      },
      "source": [
        "## IMDB 리뷰 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx9zBvOfGCos"
      },
      "source": [
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7nW2BL_GNdj",
        "outputId": "0206e016-053f-47c0-dd64-ecb1460dd489"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('IMDb_Reviews.csv', <http.client.HTTPMessage at 0x7f25d4a61ad0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed81z6BgGO31",
        "outputId": "3cc00618-ead8-42ff-99e1-e501f27a931c"
      },
      "source": [
        "train_df = pd.read_csv('IMDb_Reviews.csv')\n",
        "train_df['review'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    My family and I normally do not watch local mo...\n",
              "1    Believe it or not, this was at one time the wo...\n",
              "2    After some internet surfing, I found the \"Home...\n",
              "3    One of the most unheralded great works of anim...\n",
              "4    It was the Sixties, and anyone with long hair ...\n",
              "Name: review, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4TT44gVGSXA",
        "outputId": "b459f4fd-7621-464a-8d8b-ae36736ceb99"
      },
      "source": [
        "print('리뷰 개수 :',len(train_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "리뷰 개수 : 50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLa4j8fHGcmR"
      },
      "source": [
        "센텐스피스의 입력으로 사용하기 위해서 데이터프레임을 txt 파일로 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etc88In6GVHT"
      },
      "source": [
        "with open('imdb_review.txt', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(train_df['review']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqmcgaUkGfe8"
      },
      "source": [
        "센텐스피스로 단어 집합과 각 단어에 고유한 정수를 부여"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFsyE7qwGfWC"
      },
      "source": [
        "spm.SentencePieceTrainer.Train('--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl08qGvIGtmH"
      },
      "source": [
        "- input : 학습시킬 파일\n",
        "- model_prefix : 만들어질 모델 이름\n",
        "- vocab_size : 단어 집합의 크기\n",
        "- model_type : 사용할 모델 (unigram(default), bpe, char, word)\n",
        "- max_sentence_length: 문장의 최대 길이\n",
        "- pad_id, pad_piece: pad token id, 값\n",
        "- unk_id, unk_piece: unknown token id, 값\n",
        "- bos_id, bos_piece: begin of sentence token id, 값\n",
        "- eos_id, eos_piece: end of sequence token id, 값\n",
        "- user_defined_symbols: 사용자 정의 토큰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmoTTvUyHHyV"
      },
      "source": [
        "vocab 파일에서 학습된 서브워드들을 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "_mfiofOBGh8h",
        "outputId": "6d6af4a5-5fd0-49d2-fa80-2a775b3e79ec"
      },
      "source": [
        "vocab_list = pd.read_csv('imdb.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "vocab_list.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2300</th>\n",
              "      <td>▁points</td>\n",
              "      <td>-2297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3315</th>\n",
              "      <td>▁investig</td>\n",
              "      <td>-3312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4100</th>\n",
              "      <td>herent</td>\n",
              "      <td>-4097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4836</th>\n",
              "      <td>▁Cent</td>\n",
              "      <td>-4833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3198</th>\n",
              "      <td>▁Fran</td>\n",
              "      <td>-3195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2418</th>\n",
              "      <td>▁crime</td>\n",
              "      <td>-2415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>553</th>\n",
              "      <td>▁There</td>\n",
              "      <td>-550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1853</th>\n",
              "      <td>▁shock</td>\n",
              "      <td>-1850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3016</th>\n",
              "      <td>elly</td>\n",
              "      <td>-3013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4175</th>\n",
              "      <td>▁repeated</td>\n",
              "      <td>-4172</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              0     1\n",
              "2300    ▁points -2297\n",
              "3315  ▁investig -3312\n",
              "4100     herent -4097\n",
              "4836      ▁Cent -4833\n",
              "3198      ▁Fran -3195\n",
              "2418     ▁crime -2415\n",
              "553      ▁There  -550\n",
              "1853     ▁shock -1850\n",
              "3016       elly -3013\n",
              "4175  ▁repeated -4172"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj9gVi0SHEId"
      },
      "source": [
        "model 파일을 로드하여 단어 시퀀스를 정수 시퀀스로 바꾸는 인코딩 작업이나 반대로 변환하는 디코딩 작업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGVjsLHRGd3x",
        "outputId": "aa210332-1691-41db-e056-938ad6b2745e"
      },
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "vocab_file = \"imdb.model\"\n",
        "sp.load(vocab_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7wMs5E3HRkF",
        "outputId": "8bf841af-f925-44f1-e7ce-7231cd507e54"
      },
      "source": [
        "# 모델 테스트\n",
        "lines = [\n",
        "  \"I didn't at all think of it this way.\",\n",
        "  \"I have waited a long time for someone to film\"\n",
        "]\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  print(sp.encode_as_pieces(line))\n",
        "  print(sp.encode_as_ids(line))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I didn't at all think of it this way.\n",
            "['▁I', '▁didn', \"'\", 't', '▁at', '▁all', '▁think', '▁of', '▁it', '▁this', '▁way', '.']\n",
            "[41, 623, 4950, 4926, 138, 169, 378, 30, 58, 73, 413, 4945]\n",
            "\n",
            "I have waited a long time for someone to film\n",
            "['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film']\n",
            "[41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A945cQkcHbHF",
        "outputId": "cd57d4df-8956-4501-b990-7e997c2f3ff4"
      },
      "source": [
        "# 정수 시퀀스로부터 문장으로 변환 _ 디코딩\n",
        "sp.DecodeIds([41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I have waited a long time for someone to film'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqXe_T6vHmeg",
        "outputId": "641062f3-74ae-4365-f7e1-5dc108865ae5"
      },
      "source": [
        "# encode : 문장으로부터 인자값에 따라서 정수 시퀀스 또는 서브워드 시퀀스로 변환 가능\n",
        "print(sp.encode('I have waited a long time for someone to film', out_type=str))\n",
        "print(sp.encode('I have waited a long time for someone to film', out_type=int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film']\n",
            "[41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr1-UBSNKQg2"
      },
      "source": [
        "## 네이버 영화 리뷰 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kurZMZ8hHyK7"
      },
      "source": [
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import urllib.request\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "YfPYGQSGTEO-",
        "outputId": "99a2c7a1-5313-459d-841d-ad99b578c366"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
        "naver_df = pd.read_table('ratings.txt')\n",
        "print('리뷰 개수 :',len(naver_df))\n",
        "naver_df.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "리뷰 개수 : 200000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8112052</td>\n",
              "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8132799</td>\n",
              "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4655635</td>\n",
              "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                           document  label\n",
              "0  8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
              "1  8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
              "2  4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fepEP6S5TIOG",
        "outputId": "de2c7dd8-e9dc-45c0-d992-6ab3124be6d5"
      },
      "source": [
        "# 전처리 과정\n",
        "print(naver_df.isnull().values.any())\n",
        "naver_df = naver_df.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "print(naver_df.isnull().values.any()) # Null 값이 존재하는지 확인\n",
        "print('리뷰 개수 :',len(naver_df)) # 리뷰 개수 출력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "리뷰 개수 : 199992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2BH7ErtTbBO"
      },
      "source": [
        "센텐스피스의 입력으로 사용하기 위해서 데이터프레임을 txt 파일로 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqaLbbmUTZd8"
      },
      "source": [
        "with open('naver_review.txt', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(naver_df['document']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO5cb-rMTgH0"
      },
      "source": [
        "spm.SentencePieceTrainer.Train('--input=naver_review.txt --model_prefix=naver --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVdGmFTsTkfh"
      },
      "source": [
        "- input : 학습시킬 파일\n",
        "- model_prefix : 만들어질 모델 이름\n",
        "- vocab_size : 단어 집합의 크기\n",
        "- model_type : 사용할 모델 (unigram(default), bpe, char, word)\n",
        "- max_sentence_length: 문장의 최대 길이\n",
        "- pad_id, pad_piece: pad token id, 값\n",
        "- unk_id, unk_piece: unknown token id, 값\n",
        "- bos_id, bos_piece: begin of sentence token id, 값\n",
        "- eos_id, eos_piece: end of sequence token id, 값\n",
        "- user_defined_symbols: 사용자 정의 토큰  \n",
        "\n",
        "**vocab 생성이 완료되면 `naver.model, naver.vocab` 파일 두개가 생성**  \n",
        "- .vocab 에서 학습된 subwords를 확인  \n",
        "- model 파일을 로드하여 단어 시퀀스를 정수 시퀀스로 바꾸는 인코딩 작업이나 반대로 변환하는 디코딩 작업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "a221n19PThQM",
        "outputId": "579d0ae6-9c4e-4abb-a5be-d22daa3feb69"
      },
      "source": [
        "# naver.vocab 에서 학습된 subwords를 확인\n",
        "vocab_list = pd.read_csv('naver.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "len(vocab_list) #설정한대로 5000개의 서브워드가 단어 집합에 존재\n",
        "vocab_list.sample(10) # 샘플 출력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1608</th>\n",
              "      <td>▁탄탄</td>\n",
              "      <td>-1605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1069</th>\n",
              "      <td>▁옛날</td>\n",
              "      <td>-1066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>652</th>\n",
              "      <td>내용</td>\n",
              "      <td>-649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2629</th>\n",
              "      <td>▁미스</td>\n",
              "      <td>-2626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4093</th>\n",
              "      <td>랫</td>\n",
              "      <td>-4090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2347</th>\n",
              "      <td>성애</td>\n",
              "      <td>-2344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2617</th>\n",
              "      <td>디어</td>\n",
              "      <td>-2614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2642</th>\n",
              "      <td>▁채널</td>\n",
              "      <td>-2639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336</th>\n",
              "      <td>▁마음</td>\n",
              "      <td>-333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2260</th>\n",
              "      <td>▁삶을</td>\n",
              "      <td>-2257</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1\n",
              "1608  ▁탄탄 -1605\n",
              "1069  ▁옛날 -1066\n",
              "652    내용  -649\n",
              "2629  ▁미스 -2626\n",
              "4093    랫 -4090\n",
              "2347   성애 -2344\n",
              "2617   디어 -2614\n",
              "2642  ▁채널 -2639\n",
              "336   ▁마음  -333\n",
              "2260  ▁삶을 -2257"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2Q3UHvRUVkM",
        "outputId": "b78a7e98-9f15-449e-e54e-b26699a22f5a"
      },
      "source": [
        "# model 파일을 로드하여 단어 시퀀스를 정수 시퀀스로 바꾸는 인코딩 작업이나 반대로 변환하는 디코딩 작업\n",
        "sp = spm.SentencePieceProcessor()\n",
        "vocab_file = \"naver.model\"\n",
        "sp.load(vocab_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LkprYKbU5cR",
        "outputId": "6ef0683f-cc86-4a42-eced-3b4e3ede337d"
      },
      "source": [
        "#모델 평가\n",
        "lines = [\n",
        "  \"뭐 이딴 것도 영화냐.\",\n",
        "  \"진짜 최고의 영화입니다 ㅋㅋ\",\n",
        "]\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  print(sp.encode_as_pieces(line)) # 서브 토큰화\n",
        "  print(sp.encode_as_ids(line)) # 정수 인덱싱\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "뭐 이딴 것도 영화냐.\n",
            "['▁뭐', '▁이딴', '▁것도', '▁영화냐', '.']\n",
            "[132, 966, 1296, 2590, 3276]\n",
            "\n",
            "진짜 최고의 영화입니다 ㅋㅋ\n",
            "['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\n",
            "[54, 200, 821, 85]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNcEyeZNVFS1",
        "outputId": "b106569b-8dec-4bef-dfe1-bb52ec5c3bca"
      },
      "source": [
        "print('단어 집합의 크기: ',sp.GetPieceSize())\n",
        "print('정수로부터 맵핑되는 서브 워드로 변환:\\t[정수: 430] => ', sp.IdToPiece(430))\n",
        "print('서브워드로부터 맵핑되는 정수로 변환: \\t[워드:\"스럽\"]=> ',sp.PieceToId('스럽'))\n",
        "print('정수 시퀀스로부터 문장으로 변환: ', sp.DecodeIds([54, 200, 821, 85]))\n",
        "print('서브워드 시퀀스로부터 문장으로 변환: ', sp.DecodePieces(['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']))\n",
        "print()\n",
        "print('문장으로부터 인자값에 따라서 정수 시퀀스 또는 서브워드 시퀀스로 변환: ',)\n",
        "print(sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=str))\n",
        "print(sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기:  5000\n",
            "정수로부터 맵핑되는 서브 워드로 변환:\t[정수: 430] =>  스럽\n",
            "서브워드로부터 맵핑되는 정수로 변환: \t[워드:\"스럽\"]=>  430\n",
            "정수 시퀀스로부터 문장으로 변환:  진짜 최고의 영화입니다 ᄏᄏ\n",
            "서브워드 시퀀스로부터 문장으로 변환:  진짜 최고의 영화입니다 ᄏᄏ\n",
            "\n",
            "문장으로부터 인자값에 따라서 정수 시퀀스 또는 서브워드 시퀀스로 변환: \n",
            "['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\n",
            "[54, 200, 821, 85]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R74kqu7uVajO"
      },
      "source": [
        "# Word Piece Model(WPM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXUYPsNaXoAv"
      },
      "source": [
        "- 텐서플로우를 통해 사용할 수 있는 서브워드 토크나이저\n",
        "- BPE와 유사한 알고리즘\n",
        "- 패키지를 통해 쉽게 단어들을 서브워드들로 분리할 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0st8cipXv11"
      },
      "source": [
        "## 네이버 영화 리뷰 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8b0gaqtXmUm"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import urllib.request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HLwnK6KXzuV"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "train_data = pd.read_table('ratings_train.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGkd6uCBX20v",
        "outputId": "7ca5efb6-4dd9-475f-82e3-389910550ab4"
      },
      "source": [
        "#전처리1\n",
        "print(train_data.isnull().sum())\n",
        "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id          0\n",
            "document    5\n",
            "label       0\n",
            "dtype: int64\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeO0o9pwYMCE"
      },
      "source": [
        "tfds.features.text.SubwordTextEncoder.build_from_corpus의 인자로 네이버 영화 리뷰 데이터를 넣어서, 서브워드들로 이루어진 단어 집합(Vocabulary)를 생성하고, 각 서브워드에 고유한 정수를 부여"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T478-XtqX3vr",
        "outputId": "5f47dee5-40b4-4b33-9fd0-d22da91967bd"
      },
      "source": [
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_data['document'], target_vocab_size=2**13)\n",
        "print(tokenizer.subwords[:100]) # 토큰화 된 100개의 서브워드들을 출력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['. ', '..', '영화', '이_', '...', '의_', '는_', '도_', '다', ', ', '을_', '고_', '은_', '가_', '에_', '.. ', '한_', '너무_', '정말_', '를_', '고', '게_', '영화_', '지', '... ', '진짜_', '이', '다_', '요', '만_', '? ', '과_', '나', '가', '서_', '지_', '로_', '으로_', '아', '어', '....', '음', '한', '수_', '와_', '도', '네', '그냥_', '나_', '더_', '왜_', '이런_', '면_', '기', '하고_', '보고_', '하는_', '서', '좀_', '리', '자', '스', '안', '! ', '에서_', '영화를_', '미', 'ㅋㅋ', '네요', '시', '주', '라', '는', '오', '없는_', '에', '해', '사', '!!', '영화는_', '마', '잘_', '수', '영화가_', '만', '본_', '로', '그_', '지만_', '대', '은', '비', '의', '일', '개', '있는_', '없다', '함', '구', '하']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xuaCePYZBoc"
      },
      "source": [
        "encode()를 통해 임의로 선택한 21번째 샘플을 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Gqh6TcYQBd",
        "outputId": "1a7a5d74-9dfd-4238-c09e-4c1ce658d862"
      },
      "source": [
        "print('Tokenized sample question: {}'.format(tokenizer.encode(train_data['document'][20])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized sample question: [669, 4700, 17, 1749, 8, 96, 131, 1, 48, 2239, 4, 7466, 32, 1274, 2655, 7, 80, 749, 1254]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIWDF599Y9vs"
      },
      "source": [
        "decode()를 통해서 다시 역으로 디코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkpIUJdpZOq1",
        "outputId": "8c60ac27-0c83-4a75-972e-5d5280ed9c4c"
      },
      "source": [
        "sample_string = train_data['document'][21]\n",
        "\n",
        "# 인코딩한 결과를 tokenized_string에 저장\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
        "\n",
        "# 이를 다시 디코딩\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('기존 문장: {}'.format(original_string))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정수 인코딩 후의 문장 [570, 892, 36, 584, 159, 7091, 201]\n",
            "기존 문장: 보면서 웃지 않는 건 불가능하다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02bRFCT0ZYiv",
        "outputId": "39551f00-f1e7-43f6-9fe9-928baa262899"
      },
      "source": [
        "# 임의의 문장으로 테스트 해보기\n",
        "sample_string = '이 영화 굉장히 재밌다 킄핫핫ㅎ'\n",
        "\n",
        "# 인코딩한 결과를 tokenized_string에 저장\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
        "\n",
        "# 이를 다시 디코딩\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('기존 문장: {}'.format(original_string))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정수 인코딩 후의 문장 [4, 23, 1364, 2157, 8235, 8128, 8130, 8235, 8147, 8169, 8235, 8147, 8169, 393]\n",
            "기존 문장: 이 영화 굉장히 재밌다 킄핫핫ㅎ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTy2T20XZke_"
      },
      "source": [
        "기존 훈련 데이터에 없을만한 '킄핫핫'  \n",
        "서브워드텍스트인코더는 이 경우 음절 이하 단위로 분리하고, 또한 정상적으로 디코딩"
      ]
    }
  ]
}